{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075240e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189e202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147f2728",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2025-08-19\"\n",
    "end_date = \"2025-08-21\"\n",
    "folder_name = f\"v2_data//pred_data//test_pred_v2_{start_date}_{end_date}//training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07685532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path for .csv where training data is stored.\n",
    "training_path = f\"{folder_name}//training_data.csv\"\n",
    "diff_training_path = f\"{folder_name}//differential_training_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f06681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "training_data = pd.read_csv(training_path)\n",
    "training_data = training_data.round(2)\n",
    "\n",
    "diff_training_data = pd.read_csv(diff_training_path)\n",
    "diff_training_data = diff_training_data.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a8c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_issues(df):\n",
    "    \"\"\"Check for common missing/null/error values in the DataFrame and print summary. Also print rows with errors.\"\"\"\n",
    "    import numpy as np\n",
    "    print(\"--- Data Issues Summary ---\")\n",
    "    # Check for NaN values\n",
    "    nan_counts = df.isna().sum()\n",
    "    if nan_counts.any():\n",
    "        print(\"NaN values found:\")\n",
    "        print(nan_counts[nan_counts > 0])\n",
    "    else:\n",
    "        print(\"No NaN values found.\")\n",
    "\n",
    "    # Check for infinite values\n",
    "    inf_counts = np.isinf(df.select_dtypes(include=[float, int])).sum()\n",
    "    if inf_counts.any():\n",
    "        print(\"Infinite values found:\")\n",
    "        print(inf_counts[inf_counts > 0])\n",
    "    else:\n",
    "        print(\"No infinite values found.\")\n",
    "\n",
    "    # Check for string 'inf', '-inf', 'nan', 'None', or empty string\n",
    "    error_strings = ['inf', '-inf', 'nan', 'None', '']\n",
    "    error_rows = set()\n",
    "    for col in df.select_dtypes(include=[object]).columns:\n",
    "        for err in error_strings:\n",
    "            mask = (df[col] == err)\n",
    "            count = mask.sum()\n",
    "            if count > 0:\n",
    "                print(f\"Column '{col}' has {count} occurrences of '{err}'\")\n",
    "                error_rows.update(df[mask].index.tolist())\n",
    "\n",
    "    # Collect all error rows (NaN, inf, error strings)\n",
    "    nan_rows = set(df[df.isna().any(axis=1)].index.tolist())\n",
    "    inf_rows = set(df[np.isinf(df.select_dtypes(include=[float, int])).any(axis=1)].index.tolist())\n",
    "    all_error_rows = nan_rows.union(inf_rows).union(error_rows)\n",
    "\n",
    "    print(\"--- End of Data Issues Summary ---\")\n",
    "    if all_error_rows:\n",
    "        print(f\"\\nDetailed view of rows with data errors ({len(all_error_rows)} rows) saved to .csv:\")\n",
    "        df.loc[sorted(all_error_rows)].to_csv(f\"{df}_data_errors.csv\", index=False)\n",
    "    else:\n",
    "        print(\"No rows with data errors found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f85c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Issues Summary ---\n",
      "No NaN values found.\n",
      "No infinite values found.\n",
      "--- End of Data Issues Summary ---\n",
      "No rows with data errors found.\n",
      "--- Data Issues Summary ---\n",
      "No NaN values found.\n",
      "No infinite values found.\n",
      "--- End of Data Issues Summary ---\n",
      "No rows with data errors found.\n"
     ]
    }
   ],
   "source": [
    "check_data_issues(training_data)\n",
    "check_data_issues(diff_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b6357",
   "metadata": {},
   "source": [
    "Create training split. Currently 25% test, team_o_win prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbdb1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_data(train_data):\n",
    "    \"\"\" Split trianing data into features and targets\"\"\"\n",
    "    y = training_data['team_0_win']\n",
    "    X = training_data.drop(columns=['team_0_win'])\n",
    "\n",
    "    # Split  data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a77f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_training_data(training_data)\n",
    "# X_train, X_test, y_train, y_test = prep_training_data(diff_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76490de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, X_test, y_train, y_test, params, model_type):\n",
    "\n",
    "    # Create unique model identifier\n",
    "    ## {model_type} {date_time} {random_state}\n",
    "    from datetime import datetime\n",
    "\n",
    "    date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_id = f\"{model_type}_{date_time}_{params.get('random_state', 42)}\"\n",
    "\n",
    "    # Initialize the model with these starting parameters\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=params.get(\"n_estimators\", 100),\n",
    "        max_depth=params.get(\"max_depth\", None),\n",
    "        min_samples_split=params.get(\"min_samples_split\", 2),\n",
    "        min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
    "        max_features=params.get(\"max_features\", \"sqrt\"),\n",
    "        random_state=params.get(\"random_state\", 42),\n",
    "        n_jobs=-1                # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Print Model params and start\n",
    "    print(f\"Starting Training, Modelid = {model_id}\")\n",
    "    print(\"Training Random Forest with parameters:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "    return rf_model, model_id, y_pred\n",
    "\n",
    "def evaluate_model(model, y_test, y_pred, X)-> dict:\n",
    "    \"\"\"Create a report evaluating model passed, returns dict of report data\"\"\"\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred).tolist()\n",
    "    feature_importance = pd.DataFrame(\n",
    "        {'feature': X, 'importance': model.feature_importances_},\n",
    "    ).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nTop 25 important features:\")\n",
    "    print(feature_importance.head(25))\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'feature_importance': feature_importance.to_dict(orient='records')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d9fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Params\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": None,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"random_state\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c508b60",
   "metadata": {},
   "source": [
    "# Change per run! Set model ID and Folder to save data to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21753511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name model for ID\n",
    "model_type = \"rf_std_v2\"\n",
    "\n",
    "#### MAKE SURE TO CHANGE THE FOLDER!!!!! #####\n",
    "model_folder = f\"models//8.24.25//{model_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ed922bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training, Modelid = rf_std_v2_20250824_124805_42\n",
      "Training Random Forest with parameters:\n",
      "n_estimators: 100\n",
      "max_depth: None\n",
      "min_samples_split: 2\n",
      "min_samples_leaf: 1\n",
      "max_features: sqrt\n",
      "random_state: 42\n"
     ]
    }
   ],
   "source": [
    "model, model_id, y_pred = train_random_forest(X_train, X_test, y_train, y_test, params, model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab299e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "012f3188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5872\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.58      0.50      0.54       105\n",
      "           Y       0.59      0.66      0.62       113\n",
      "\n",
      "    accuracy                           0.59       218\n",
      "   macro avg       0.59      0.58      0.58       218\n",
      "weighted avg       0.59      0.59      0.58       218\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[53 52]\n",
      " [38 75]]\n",
      "\n",
      "Top 25 important features:\n",
      "                               feature  importance\n",
      "153        ph_win_rate_ratio_min_Team0    0.016596\n",
      "157       ph_win_rate_ratio_mean_Team0    0.015258\n",
      "158       ph_win_rate_ratio_mean_Team1    0.013649\n",
      "142             ph_win_rate_mean_Team1    0.013378\n",
      "137              ph_win_rate_min_Team0    0.013116\n",
      "86              ph_kd_ratio_mean_Team1    0.011736\n",
      "0                             match_id    0.011588\n",
      "101     ph_avg_match_length_mean_Team0    0.011248\n",
      "141             ph_win_rate_mean_Team0    0.011101\n",
      "105  ph_avg_damage_per_match_min_Team0    0.011092\n",
      "33            ph_time_played_min_Team0    0.010797\n",
      "122          ph_damage_ratio_min_Team1    0.010642\n",
      "100      ph_avg_match_length_max_Team1    0.009859\n",
      "102     ph_avg_match_length_mean_Team1    0.009625\n",
      "83               ph_kd_ratio_max_Team0    0.009579\n",
      "108  ph_avg_damage_per_match_max_Team1    0.009564\n",
      "97       ph_avg_match_length_min_Team0    0.009537\n",
      "7     p_total_matches_played_std_Team0    0.009521\n",
      "85              ph_kd_ratio_mean_Team0    0.009444\n",
      "84               ph_kd_ratio_max_Team1    0.008955\n",
      "29        ph_matches_played_mean_Team0    0.008899\n",
      "25         ph_matches_played_min_Team0    0.008848\n",
      "119       h_damage_per_match_std_Team0    0.008809\n",
      "66               ph_total_kd_min_Team1    0.008761\n",
      "88               ph_kd_ratio_std_Team1    0.008759\n"
     ]
    }
   ],
   "source": [
    "report = evaluate_model(model, y_test, y_pred,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd108fc",
   "metadata": {},
   "source": [
    "Save Model, parameters, trianing data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7cef30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, params, folder, model_id,feature_names = X):\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import joblib, json, platform, os\n",
    "\n",
    "    # create subfolders\n",
    "    os.makedirs(f\"{folder}/samples\", exist_ok=True)\n",
    "\n",
    "    joblib.dump(model, f\"{folder}/model.joblib\")\n",
    "\n",
    "    print(f\"Model saved to {folder}\")\n",
    "\n",
    "    json.dump(params, open(f\"{folder}/params.json\",\"w\"))\n",
    "\n",
    "    json.dump({\n",
    "    \"run_id\": model_id, \"python\": platform.python_version(),\n",
    "    \"feature_names\": feature_names,\"random_state\": params.get(\"random_state\", 42),\n",
    "    }, open(f\"{folder}/meta.json\",\"w\"))\n",
    "\n",
    "def save_report(training_data, model_id, model_folder, results):\n",
    "    import json\n",
    "\n",
    "    training_data.to_csv(f\"{model_folder}/{model_id}_training_data.csv\")\n",
    "\n",
    "    with open(f\"{model_folder}/{model_id}_results.txt\", \"w\") as f:\n",
    "        f.write(f\"Accuracy: {results['accuracy']}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(json.dumps(results['classification_report'], indent=2))\n",
    "        f.write(\"\\n\\nConfusion Matrix:\\n\")\n",
    "        f.write(str(results['confusion_matrix']))\n",
    "        f.write(\"\\n\\nTop Features:\\n\")\n",
    "        for feat in results['feature_importance'][:25]:\n",
    "            f.write(f\"{feat['feature']}: {feat['importance']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323382f7",
   "metadata": {},
   "source": [
    "Save Model and trianing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccf051ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models//8.24.25//rf_std_v2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = X.tolist()\n",
    "\n",
    "save_model(model, params, model_folder, model_id, features)\n",
    "save_report(training_data, model_id, model_folder, report)\n",
    "\n",
    "train = X_train.copy()\n",
    "train[\"target\"] = y_train\n",
    "train.to_csv(f\"{model_folder}/samples/Xy_train.csv\", index=False)\n",
    "\n",
    "test = X_test.copy()\n",
    "test[\"target\"] = y_test\n",
    "test.to_csv(f\"{model_folder}/samples/Xy_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
