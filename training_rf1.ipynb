{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075240e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "189e202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "07685532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path for .csv where training data is stored.\n",
    "training_path = \"v2_data/training_data_differentials_2025-08-17.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0f06681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "training_data = pd.read_csv(training_path)\n",
    "training_data = training_data.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "70a8c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_issues(df):\n",
    "    \"\"\"Check for common missing/null/error values in the DataFrame and print summary. Also print rows with errors.\"\"\"\n",
    "    import numpy as np\n",
    "    print(\"--- Data Issues Summary ---\")\n",
    "    # Check for NaN values\n",
    "    nan_counts = df.isna().sum()\n",
    "    if nan_counts.any():\n",
    "        print(\"NaN values found:\")\n",
    "        print(nan_counts[nan_counts > 0])\n",
    "    else:\n",
    "        print(\"No NaN values found.\")\n",
    "\n",
    "    # Check for infinite values\n",
    "    inf_counts = np.isinf(df.select_dtypes(include=[float, int])).sum()\n",
    "    if inf_counts.any():\n",
    "        print(\"Infinite values found:\")\n",
    "        print(inf_counts[inf_counts > 0])\n",
    "    else:\n",
    "        print(\"No infinite values found.\")\n",
    "\n",
    "    # Check for string 'inf', '-inf', 'nan', 'None', or empty string\n",
    "    error_strings = ['inf', '-inf', 'nan', 'None', '']\n",
    "    error_rows = set()\n",
    "    for col in df.select_dtypes(include=[object]).columns:\n",
    "        for err in error_strings:\n",
    "            mask = (df[col] == err)\n",
    "            count = mask.sum()\n",
    "            if count > 0:\n",
    "                print(f\"Column '{col}' has {count} occurrences of '{err}'\")\n",
    "                error_rows.update(df[mask].index.tolist())\n",
    "\n",
    "    # Collect all error rows (NaN, inf, error strings)\n",
    "    nan_rows = set(df[df.isna().any(axis=1)].index.tolist())\n",
    "    inf_rows = set(df[np.isinf(df.select_dtypes(include=[float, int])).any(axis=1)].index.tolist())\n",
    "    all_error_rows = nan_rows.union(inf_rows).union(error_rows)\n",
    "\n",
    "    print(\"--- End of Data Issues Summary ---\")\n",
    "    if all_error_rows:\n",
    "        print(f\"\\nDetailed view of rows with data errors ({len(all_error_rows)} rows) saved to .csv:\")\n",
    "        df.loc[sorted(all_error_rows)].to_csv(\"data_errors.csv\", index=False)\n",
    "    else:\n",
    "        print(\"No rows with data errors found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3f85c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Issues Summary ---\n",
      "No NaN values found.\n",
      "No infinite values found.\n",
      "--- End of Data Issues Summary ---\n",
      "No rows with data errors found.\n"
     ]
    }
   ],
   "source": [
    "check_data_issues(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b6357",
   "metadata": {},
   "source": [
    "Create training split. Currently 25% test, team_o_win prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a77f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data\n",
    "y = training_data['team_0_win']\n",
    "X = training_data.drop(columns=['team_0_win'])\n",
    "\n",
    "# Split  data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "76490de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, X_test, y_train, y_test, params, model_type):\n",
    "\n",
    "    # Create unique model identifier\n",
    "    ## {model_type} {date_time} {random_state}\n",
    "    from datetime import datetime\n",
    "\n",
    "    date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_id = f\"{model_type}_{date_time}_{params.get('random_state', 42)}\"\n",
    "\n",
    "    # Initialize the model with these starting parameters\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=params.get(\"n_estimators\", 100),\n",
    "        max_depth=params.get(\"max_depth\", None),\n",
    "        min_samples_split=params.get(\"min_samples_split\", 2),\n",
    "        min_samples_leaf=params.get(\"min_samples_leaf\", 1),\n",
    "        max_features=params.get(\"max_features\", \"sqrt\"),\n",
    "        random_state=params.get(\"random_state\", 42),\n",
    "        n_jobs=-1                # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Print Model params and start\n",
    "    print(f\"Starting Training, Modelid = {model_id}\")\n",
    "    print(\"Training Random Forest with parameters:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "    return rf_model, model_id, y_pred\n",
    "\n",
    "def evaluate_model(model, y_test, y_pred)-> dict:\n",
    "    \"\"\"Create a report evaluating model passed, returns dict of report data\"\"\"\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred).tolist()\n",
    "    feature_importance = pd.DataFrame(\n",
    "        {'feature': X.columns, 'importance': model.feature_importances_},\n",
    "    ).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nTop 25 important features:\")\n",
    "    print(feature_importance.head(25))\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'feature_importance': feature_importance.to_dict(orient='records')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a4d9fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Params\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": None,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"random_state\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c508b60",
   "metadata": {},
   "source": [
    "# Change per run! Set model ID and Folder to save data to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "21753511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name model for ID\n",
    "model_type = \"rf_quantiles_diff\"\n",
    "\n",
    "#### MAKE SURE TO CHANGE THE FOLDER!!!!! #####\n",
    "model_folder = \"models/8.14.25_rf_1_quantiles_diff/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5ed922bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training, Modelid = rf_quantiles_diff_20250817_134123_42\n",
      "Training Random Forest with parameters:\n",
      "n_estimators: 100\n",
      "max_depth: None\n",
      "min_samples_split: 2\n",
      "min_samples_leaf: 1\n",
      "max_features: sqrt\n",
      "random_state: 42\n"
     ]
    }
   ],
   "source": [
    "model, model_id, y_pred = train_random_forest(X_train, X_test, y_train, y_test, params, model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "012f3188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6854\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.66      0.75      0.70        88\n",
      "           Y       0.72      0.62      0.67        90\n",
      "\n",
      "    accuracy                           0.69       178\n",
      "   macro avg       0.69      0.69      0.68       178\n",
      "weighted avg       0.69      0.69      0.68       178\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[66 22]\n",
      " [34 56]]\n",
      "\n",
      "Top 25 important features:\n",
      "                             feature  importance\n",
      "62        ph_win_rate_ratio_min_diff    0.051094\n",
      "63        ph_win_rate_ratio_max_diff    0.051067\n",
      "64        ph_win_rate_ratio_q25_diff    0.040082\n",
      "65     ph_win_rate_ratio_median_diff    0.031285\n",
      "66        ph_win_rate_ratio_q75_diff    0.024454\n",
      "37              ph_total_kd_min_diff    0.020388\n",
      "28        ph_damage_per_min_max_diff    0.019205\n",
      "0                         Unnamed: 0    0.019161\n",
      "2       p_total_time_played_min_diff    0.018646\n",
      "27        ph_damage_per_min_min_diff    0.017959\n",
      "4       p_total_time_played_q25_diff    0.017773\n",
      "41              ph_total_kd_q75_diff    0.016755\n",
      "29        ph_damage_per_min_q25_diff    0.016208\n",
      "1                           match_id    0.016040\n",
      "14                 ph_kills_q25_diff    0.015925\n",
      "5    p_total_time_played_median_diff    0.015882\n",
      "53  ph_damage_per_min_ratio_max_diff    0.015424\n",
      "42              ph_kd_ratio_min_diff    0.015149\n",
      "31        ph_damage_per_min_q75_diff    0.015139\n",
      "19                ph_deaths_q25_diff    0.015007\n",
      "52  ph_damage_per_min_ratio_min_diff    0.014909\n",
      "46              ph_kd_ratio_q75_diff    0.014346\n",
      "3       p_total_time_played_max_diff    0.014223\n",
      "30     ph_damage_per_min_median_diff    0.014197\n",
      "39              ph_total_kd_q25_diff    0.014097\n"
     ]
    }
   ],
   "source": [
    "report = evaluate_model(model, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd108fc",
   "metadata": {},
   "source": [
    "Save Model, parameters, trianing data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a7cef30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, params, folder, model_id,feature_names = X.columns):\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import joblib, json, platform, os\n",
    "\n",
    "    # create subfolders\n",
    "    os.makedirs(f\"{folder}/samples\", exist_ok=True)\n",
    "\n",
    "    joblib.dump(model, f\"{folder}/model.joblib\")\n",
    "\n",
    "    print(f\"Model saved to {folder}\")\n",
    "\n",
    "    json.dump(params, open(f\"{folder}/params.json\",\"w\"))\n",
    "\n",
    "    json.dump({\n",
    "    \"run_id\": model_id, \"python\": platform.python_version(),\n",
    "    \"feature_names\": feature_names,\"random_state\": params.get(\"random_state\", 42),\n",
    "    }, open(f\"{folder}/meta.json\",\"w\"))\n",
    "\n",
    "def save_report(training_data, model_id, model_folder, results):\n",
    "    import json\n",
    "\n",
    "    training_data.to_csv(f\"{model_folder}/{model_id}_training_data.csv\")\n",
    "\n",
    "    with open(f\"{model_folder}/{model_id}_results.txt\", \"w\") as f:\n",
    "        f.write(f\"Accuracy: {results['accuracy']}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(json.dumps(results['classification_report'], indent=2))\n",
    "        f.write(\"\\n\\nConfusion Matrix:\\n\")\n",
    "        f.write(str(results['confusion_matrix']))\n",
    "        f.write(\"\\n\\nTop Features:\\n\")\n",
    "        for feat in results['feature_importance'][:25]:\n",
    "            f.write(f\"{feat['feature']}: {feat['importance']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323382f7",
   "metadata": {},
   "source": [
    "Save Model and trianing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ccf051ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/8.14.25_rf_1_quantiles_diff/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = X.columns.tolist()\n",
    "\n",
    "save_model(model, params, model_folder, model_id, features)\n",
    "save_report(training_data, model_id, model_folder, report)\n",
    "\n",
    "train = X_train.copy()\n",
    "train[\"target\"] = y_train\n",
    "train.to_csv(f\"{model_folder}/samples/Xy_train.csv\", index=False)\n",
    "\n",
    "test = X_test.copy()\n",
    "test[\"target\"] = y_test\n",
    "test.to_csv(f\"{model_folder}/samples/Xy_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
